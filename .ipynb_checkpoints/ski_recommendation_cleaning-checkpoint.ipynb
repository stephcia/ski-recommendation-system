{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avant Ski"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by: Stephanie Ciaccia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skiing holds a prominent place for those seeking winter recreational activities in the United States. With its stunning mountain ranges and diverse terrain, the country boasts numerous ski resorts that cater to all skill levels, from beginners to seasoned professionals. \n",
    "\n",
    "Skiing offers a unique blend of adventure, physical activity, and natural beauty, making it a popular choice for winter enthusiasts seeking both relaxation and excitement.\n",
    "\n",
    "The ski market in the United States is thriving, contributing significantly to the economy. According to the [National Ski Areas Association (NSAA)](chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://nsaa.org/webdocs/Media_Public/IndustryStats/Historical_Skier_Days_1979_2022.pdf), approximately 60.7 million skiers and snowboarders visited 473 ski resorts in the 2021-2022 winter season."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skiing is an exhilarating winter activity enjoyed by many, but barriers such as high costs and limited accessibility often hinder people from fully experiencing its joys. Choosing the right ski resort can be overwhelming due to the multitude of options available, and existing websites lack dynamic filtering capabilities based on user preferences.\n",
    "\n",
    "To address these challenges, I'm developing Avant Ski, a ski resort recommendation app. Avant Ski simplifies the ski resort selection process by leveraging data and user preferences. With dynamic filtering features, users can personalize their search based on budget, location, amenities, and skill level. By bridging the gap between ski enthusiasts and their dream destinations, Avant Ski makes skiing accessible to a wider audience, empowering them to plan unforgettable ski trips with confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime\n",
    "import datetime\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise import Dataset, Reader, accuracy\n",
    "from surprise.prediction_algorithms import KNNWithMeans, KNNBasic, KNNBaseline, KNNWithZScore,  SVD, SVDpp, NMF, BaselineOnly, NormalPredictor\n",
    "from surprise.model_selection import GridSearchCV, cross_validate, train_test_split\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to print full rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_full(x):\n",
    "    pd.set_option('display.max_rows', len(x))\n",
    "    print(x)\n",
    "    pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_df = pd.read_csv(\"data/OnTheSnow_SkiAreaReviews_clean.csv\")\n",
    "survey_df = pd.read_csv(\"data/usa_ski_resort_survey.csv\")\n",
    "scraped_df = pd.read_csv(\"data/onthesnow_scrape_170523_cleaned.csv\")\n",
    "second_scrape = pd.read_csv(\"data/OnTheSnow_Srape_2_200523_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'scraped_cleaned/dec_4_airbnb_mean_final.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-85dde5f7e6d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#airbnb scrape four guest listings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdec_4_airbnb_mean_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scraped_cleaned/dec_4_airbnb_mean_final.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mjan_4_airbnb_mean_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scraped_cleaned/jan_4_airbnb_mean_final.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfeb_4_airbnb_mean_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scraped_cleaned/feb_4_airbnb_mean_final.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmar_4_airbnb_mean_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scraped_cleaned/mar_4_airbnb_mean_final.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'scraped_cleaned/dec_4_airbnb_mean_final.csv'"
     ]
    }
   ],
   "source": [
    "#airbnb scrape four guest listings\n",
    "dec_4_airbnb_mean_final = pd.read_csv(\"data/scraped_cleaned/dec_4_airbnb_mean_final.csv\")\n",
    "jan_4_airbnb_mean_final = pd.read_csv(\"scraped_cleaned/jan_4_airbnb_mean_final.csv\")\n",
    "feb_4_airbnb_mean_final = pd.read_csv(\"scraped_cleaned/feb_4_airbnb_mean_final.csv\")\n",
    "mar_4_airbnb_mean_final = pd.read_csv(\"scraped_cleaned/mar_4_airbnb_mean_final.csv\")\n",
    "apr_4_airbnb_mean_final = pd.read_csv(\"scraped_cleaned/apr_4_airbnb_mean_final.csv\")\n",
    "may_4_airbnb_mean_final = pd.read_csv(\"scraped_cleaned/may_4_airbnb_mean_final.csv\")\n",
    "\n",
    "#airbnb scrape two guest listings\n",
    "dec_2_airbnb_mean_final = pd.read_csv(\"scraped_cleaned/dec_2_airbnb_mean_final.csv\")\n",
    "jan_2_airbnb_mean_final = pd.read_csv(\"scraped_cleaned/jan_2_airbnb_mean_final.csv\")\n",
    "feb_2_airbnb_mean_final = pd.read_csv(\"scraped_cleaned/feb_2_airbnb_mean_final.csv\")\n",
    "mar_2_airbnb_mean_final = pd.read_csv(\"scraped_cleaned/mar_2_airbnb_mean_final.csv\")\n",
    "apr_2_airbnb_mean_final = pd.read_csv(\"scraped_cleaned/apr_2_airbnb_mean_final.csv\")\n",
    "may_2_airbnb_mean_final = pd.read_csv(\"scraped_cleaned/may_2_airbnb_mean_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#google geocoding api\n",
    "latitude_df = pd.read_csv(\"data/mountain_lat_long.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Source #1 - OnTheSnow (Kaggle)\n",
    "### User Based Filtering Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main dataset for the user based collaborative model was pulled from [Kaggle]([https://www.kaggle.com/datasets/fredkellner/onthesnow-ski-area-reviews]). The dataset includes reviews scraped from OnTheSnow, a leading website that provides information about ski resorts and snow conditions found on Kaggle. \n",
    "\n",
    "There are 18,128 reviews from 291 ski resorts in the USA. The features include:\n",
    "\n",
    "- Ski Area\n",
    "- Reviewer Name \n",
    "- Review Date\n",
    "- Review Star Rating (out of 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming columns\n",
    "new_name = ['state', 'ski_resort', 'user_name','review_date', 'rating',\n",
    "           'review'] \n",
    "\n",
    "snow_df.columns = new_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df['user_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_df['review_date'] = pd.to_datetime(snow_df['review_date'])\n",
    "survey_df['review_date'] = pd.to_datetime(survey_df['review_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_df[\"ski_resort\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_df['user_name'].value_counts().head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clean the review dataset, I had to drop the names of users that were not unique. I parsed through the dataset and continued to drop columns until only unique usernames or users with first and last names were left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = [\"anonymous_user\", \"anonymous\",\"undefined undefined\",\"Mike\", \n",
    "             \"Ben\", \"Ryan\", \"Richard\", \"Dan\", \"David\", \"Chris\", \"Rob\", \"Jeff\",\n",
    "            \"Derek\", \"Brian\", \"Matt\", \"Michael\", \"iPhone\", \"Kevin\", \"Nick\",\n",
    "            \"Jim\", \"Steve\", \"Jason\", \"Mark\", \"Joe\", \"Paul\", \"Justin\", \"Scott\",\n",
    "            \"Bob\", \"Alex\", \"Carter\", \"Dave\", \"Tim\", \"Bill\", \"Andrew\", \"John\", \"Sam\",\n",
    "            \"James\", \"Kim\", \"Craig\", \"mike\", \"jason\", \"James\", \"Sam\", \"Kim\", \"mike\", \"peter\",\n",
    "            \"Jack\", \"Adam\", \"Tom\", \"Wes\", \"Jun\", \"Steven\", \"Max\", \"Matthew\", \"Laura\", \"Felipe\",\n",
    "            \"Greg\", \"Bryan\", \"Sarah\", \"Sara\", \"Christian\", \"Ray\", \"Connor\", \"Erin\", \"Emily\",\n",
    "            \"Luke\", \"Ed\", \"Patrick\", \"kyle\", \"Ken\", \"Linda\", \"Eric\", \"Aaron\", \"Jake\",\n",
    "            \"Josh\", \"Tony\", \"Abe\", \"Frank\", \"Peter\", \"Fred\", \"Arthur\", \"Lorraine\",\n",
    "            \"Phil\", \"Sean\", \"Will\", \"Julie\", \"Jon\", \"Amy\", \"Becky\", \"Shannon\", \"brendan\",\n",
    "            \"Kathy\", \"wayne\", \"Ethan\", \"Erika\", \"Jill\", \"Zoe\", \"Rick\", \"Wyatt\",\n",
    "            \"Tyler\", \"Andrea\", \"mark\", \"john\", \"Donna\", \"Jen\", \"Braden\", \"D\", \"Bryce\",\n",
    "            \"Rich\", \"Jared\", \"Jay\", \"Ann\", \"Brandon\", \"Nicholas\",\"Martin\",\n",
    "            'Robert', 'angelino','Anonymous',\n",
    "             'ty', 'jase', 'Jesse', 'Jennifer', 'Dustin', 'Natalie',\n",
    "             'Pat', 'anonymous user', 'matt', 'George', 'Kate',\n",
    "             'Daniel','Cindy', 'Barry', 'Todd', 'Melanie', 'Drew',\n",
    "             'Andy', 'Hochard','Wayne', 'dan',\n",
    "             'Charlie', 'Vanessa','Allen', 'Austin', 'Roger',\n",
    "             'Jerry', 'Scotty', 'Anon', 'Lucas', 'Brian', 'Lee', 'Taylor',\n",
    "            'brian', 'Lisa', 'Jade', 'Spencer', 'chris', 'Jenny', 'Amanda', 'Brett',\n",
    "            'Maria', 'Holly', 'iPad', 'Sylvia', 'iPhone (2)', 'Catherine', 'Hannah', 'Wade',\n",
    "            'Larry', 'Lauren','Noah', 'Bobby', 'Don', 'Christine', 'Stephen', 'Howard',\n",
    "             'Tanner', 'Tom', 'Casey', 'Kyle', 'Michelle', 'Shelby',\n",
    "             'Benjamin', 'Erik', 'Molly', 'Johnny', 'Chuck', 'Johnny',\n",
    "             'Nathan', 'Cathy', 'Shelley', 'Mary', 'Danny', 'mitch', 'Brad', 'Tammy', 'erik',\n",
    "            'Tricia', 'Nate', 'Pete']\n",
    "\n",
    "snow_df = snow_df[snow_df['user_name'].isin(drop_list) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_df['user_name'].value_counts().head(70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming columns\n",
    "new_name = ['state', 'ski_resort', 'user_name','review_date', 'rating',\n",
    "           'review'] \n",
    "\n",
    "snow_df.columns = new_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning the usernames, I will be further narrowing down the number of users by only including users with more than 3 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the number of reviews for each user\n",
    "value_counts = snow_df['user_name'].value_counts()\n",
    "\n",
    "# selecting only users with more than three reviews\n",
    "selected_users = value_counts[value_counts > 2].index\n",
    "\n",
    "# selecting only the rows where the user_name is in the selected_users list\n",
    "cleaned_snow = snow_df[snow_df['user_name'].isin(selected_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_snow['user_name'].value_counts(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing users with more than 3 reviews dropped the number of rows/final reviews to 2200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_snow.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_snow['user_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping duplicate rows\n",
    "cleaned_snow = cleaned_snow.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ski resort name - cleaning\n",
    "\n",
    "Since the target variable is the Ski Resort I will need to clean and update the names in all datasets to ensure they are consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_snow['ski_resort'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing words to clean up resort names\n",
    "replace_snow = ['-ski-area', '-', 'resort', 'mt']\n",
    "replace_with = ['', ' ', '', 'mt.']\n",
    "\n",
    "cleaned_snow = cleaned_snow.replace(replace_snow, replace_with, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making columns titlecase\n",
    "cleaned_snow['ski_resort'] = cleaned_snow['ski_resort'].str.title()\n",
    "cleaned_snow['state'] = cleaned_snow['state'].str.title()\n",
    "cleaned_snow['ski_resort'] = cleaned_snow['ski_resort'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing values to standardize endings/specific resort names\n",
    "replace_snow = ['At', 'Mtn', 'Mt.N', 'Mt. Hood Ski Bowl', 'And', r'\\bMount\\b']\n",
    "replace_with = ['at', 'Mountain', 'Mountain', 'Mt. Hood Skibowl', 'and', 'Mt.']\n",
    "\n",
    "cleaned_snow = cleaned_snow.replace(replace_snow, replace_with, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After inspecting resort names, there were a few resorts that had the same names or very similar names. I adjusted the names, and included the state in the resort names to differentiate the names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timberline\n",
    "cleaned_snow.loc[(cleaned_snow['ski_resort'] == \"Timberline Four Seasons\") & (cleaned_snow['state'] == \"West Virginia\"), 'ski_resort'] = \"Timberline Mountain\"\n",
    "\n",
    "#crystal mountain\n",
    "cleaned_snow.loc[(cleaned_snow['ski_resort'] == \"Crystal Mountain Wa\") & (cleaned_snow['state'] == \"Washington\"), 'ski_resort'] = \"Crystal Mountain Washington\"\n",
    "cleaned_snow.loc[(cleaned_snow['ski_resort'] == \"Crystal Mountain\") & (cleaned_snow['state'] == \"Michigan\"), 'ski_resort'] = \"Crystal Mountain Michigan\"\n",
    "\n",
    "#magic mountain\n",
    "cleaned_snow.loc[(cleaned_snow['ski_resort'] == \"Magic Mountain\") & (cleaned_snow['state'] == \"Vermont\"), 'ski_resort'] = \"Magic Mountain Vermont\"\n",
    "cleaned_snow.loc[(cleaned_snow['ski_resort'] == \"Magic Mountain\") & (cleaned_snow['state'] == \"Idaho\"), 'ski_resort'] = \"Magic Mountain Idaho\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mountain_rep = ['Squaw Valley Usa',\n",
    "                'Mccauley Mountain Ski Center', 'attitash', 'Smugglers Notch',\n",
    "               'Pico Mountain at Killington', 'andes Tower Hills']\n",
    "\n",
    "mountain_new = ['Palisades Tahoe',\n",
    "               'McCauley Mountain Ski Center', 'Attitash', \"Smugglers' Notch\",\n",
    "               'Pico Mountain', 'Andes Tower Hills']\n",
    "\n",
    "cleaned_snow = cleaned_snow.replace(mountain_rep, mountain_new, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Source #1 - Survey Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A third small dataset was collected through a [google survey]([https://docs.google.com/forms/d/1ROrGEkCh40RjbHidNCqg4SCCbY3_6DFNw0VWIhTEIGs/edit#responses]) I distributed to individuals who ski, including myself.\n",
    "\n",
    "I downloaded the sheets file from google and saved it as a .csv. A few individuals did not include their name, so I gave them unique \"anon\" names.\n",
    "\n",
    "I plan to use the names of three users that I know, to analyze the results from the model to see if they align with the users preferences. For those three users, I also asked that they send me a brief summary of the key characteristics they look for when choosing ski resorts to visit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making columns titlecase\n",
    "survey_df['ski_resort'] = survey_df['ski_resort'].str.title()\n",
    "survey_df['state'] = survey_df['state'].str.title()\n",
    "survey_df['ski_resort'] = survey_df['ski_resort'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(survey_df['ski_resort'].sort_values().unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I manually parsed through the resort names and changed the names to match the names in the main dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mountain_rep = ['Crystal Mountain - Wa',\"Steven'S Pass\", 'Mammoth Mountain', 'Mammoth', 'Stratton',\n",
    "                'Mccauley Mountain', 'Taos', 'Snowmass', 'Palisades Tahoe', 'Palisades', 'Copper Mountain', 'Copper',\n",
    "                'Crested Butte', 'Mt. Rose', 'Mt Baker', 'Nordic Valley' ,'Solitude']\n",
    "mountain_rep_p = ['Crystal Mountain Washington', 'Stevens Pass', 'Mammoth', 'Mammoth Mountain','Stratton Mountain',\n",
    "                 'McCauley Mountain Ski Center', 'Taos Ski Valley', 'Aspen Snowmass', 'Palisades', 'Palisades Tahoe', 'Copper', 'Copper Mountain',\n",
    "                 'Crested Butte Mountain', 'Mt. Rose Ski Tahoe', 'Mt. Baker', 'Nordic Mountain', 'Solitude Mountain']\n",
    "\n",
    "survey_df = survey_df.replace(mountain_rep, mountain_rep_p, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing aspen resorts since all four mountains are part of snowmass\n",
    "mountain_r = ['Aspen Mountain', 'Aspen Highlands']\n",
    "\n",
    "survey_df = survey_df.replace(mountain_r, 'Aspen Snowmass', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking to see which names are different\n",
    "survey_df.loc[~survey_df['ski_resort'].isin(cleaned_snow['ski_resort']),\n",
    "                         'ski_resort'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging survey and OnTheSnow review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging survey review results and final onthesnow reviews\n",
    "final_ski_df = pd.concat([survey_df, cleaned_snow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping null values\n",
    "final_ski_df = final_ski_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping duplicates\n",
    "final_ski_df = final_ski_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ski_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Blackjack ski and Indianhead combined to form Snowriver Mountain Resort\n",
    "blackjack = ['Blackjack Ski', 'Indianhead Mountain']\n",
    "\n",
    "final_ski_df = final_ski_df.replace(blackjack, 'Snowriver Mountain Resort', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updating names of resort names that have changed\n",
    "old_name = ['Durango Mountain','Las Vegas Ski and Snowboard','Shawnee Peak', 'Suicide Six', 'Snow Summit']\n",
    "new_name = ['Purgatory Mountain','Lee Canyon','Shawnee Mountain', 'Saskadena Six', 'Big Bear']\n",
    "\n",
    "final_ski_df = final_ski_df.replace(old_name, new_name, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making dictionary for replacements to avoid doubling the names\n",
    "replacements = {\n",
    "    'Brandywine': 'Boston Mills and Brandywine',\n",
    "    'Boston Mills': 'Boston Mills and Brandywine'\n",
    "}\n",
    "\n",
    "# replacing\n",
    "final_ski_df['ski_resort'] = final_ski_df['ski_resort'].replace(replacements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updating duplicate ski resort names and saving as new resort names that include the state names\n",
    "final_ski_df.loc[(final_ski_df['ski_resort'] == \"Crystal Mountain\") & (final_ski_df['state'] == \"Washington\"), 'ski_resort'] = \"Crystal Mountain Washington\"\n",
    "final_ski_df.loc[(final_ski_df['ski_resort'] == \"Crystal Mountain\") & (final_ski_df['state'] == \"Michigan\"), 'ski_resort'] = \"Crystal Mountain Michigan\"\n",
    "final_ski_df.loc[(final_ski_df['ski_resort'] == \"Crystal Mountain \") & (final_ski_df['state'] == \"Michigan\"), 'ski_resort'] = \"Crystal Mountain Michigan\"\n",
    "final_ski_df.loc[(final_ski_df['ski_resort'] == \"Powder Ridge\") & (final_ski_df['state'] == \"Minnesota\"), 'ski_resort'] = \"Powder Ridge Minnesota\"\n",
    "final_ski_df.loc[(final_ski_df['ski_resort'] == \"Powder Ridge\") & (final_ski_df['state'] == \"Connecticut\"), 'ski_resort'] = \"Powder Ridge Connecticut\"\n",
    "\n",
    "#alpine valley\n",
    "final_ski_df.loc[(final_ski_df['ski_resort'] == \"Alpine Valley\") & (final_ski_df['state'] == \"Wisconsin\"), 'ski_resort'] = \"Alpine Valley Wisconsin\"\n",
    "final_ski_df.loc[(final_ski_df['ski_resort'] == \"Alpine Valley\") & (final_ski_df['state'] == \"Ohio\"), 'ski_resort'] = \"Alpine Valley Ohio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping Cherry Peak\n",
    "final_ski_df = final_ski_df.loc[(final_ski_df['ski_resort'] != \"Cherry Peak\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporting final cleaned dataframe for OnTheSnow scrape\n",
    "#final_ski_df.to_csv(\"cleaned_data_exports/final_review_df_final.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Source #3 - OnTheSnow Scrape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I scraped OnTheShow to pull current ski resort features for the resorts in the final merged datset. The code for this scrape was adapted from a [user on github] [(https://github.com/SijiaLai/OnTheSnow/tree/master)] and updated based on html changes and the features I wanted to pull.\n",
    "\n",
    "The code for the scraper can be found in the data folder.\n",
    "\n",
    "\n",
    "The main features I scraped:\n",
    "\n",
    "- mountain elevation\n",
    "- ticket price\n",
    "- mountain location\n",
    "- ski terrain\n",
    "- snowfall averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_df['state'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_rep = ['WI  54819', 'NV 89131', 'ID 83873', 'Pa 16440', 'NY', 'CO', 'CA 96160']\n",
    "state_with = [\"Wisconsin\", \"Nevada\", \"Idaho\", \"Pennsylvania\", \"New York\", \"Colorado\", \"California\"]\n",
    "\n",
    "scraped_df.replace(state_rep, state_with, regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_df.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping null values\n",
    "scraped_df = scraped_df.drop(columns=['gondolas_lifts_note', 'terrainNote', 'ticketpriceNote',\n",
    "                                     'senior_weekend', 'MaySnow', 'country'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below is code to replace empty ticket prices with mean values for the column. If time allowed, I would have preferred to manually parse through the null values and to replace them with information on individual ski resort's websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling empty lift ticket prices with mean\n",
    "\n",
    "def mean_ticket(column):\n",
    "    \n",
    "    #changing all column types to int\n",
    "    scraped_df[column] = scraped_df[column].astype(int)\n",
    "    \n",
    "    # finding mean values\n",
    "    mean_value = scraped_df[column].mean()\n",
    "    mean_value = int(mean_value)\n",
    "    \n",
    "    # filling 0 with mean value\n",
    "    scraped_df[column] = scraped_df[column].replace(0, mean_value)\n",
    "    \n",
    "    return scraped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a loop to loop through list of ticket prices that need to be updated\n",
    "\n",
    "ticket_prices = [\"adult_weekend\", \"junior_weekend\", \"child_weekend\", \"senior_weekday\", \"adult_weekday\",\n",
    "\"junior_weekday\", \"child_weekday\", \"adult_season\", \"junior_season\", \"child_season\"]\n",
    "\n",
    "for ticket_val in ticket_prices:\n",
    "    mean_ticket(ticket_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the city into zipcode and city column\n",
    "scraped_df[['zipcode', 'city']] = scraped_df['city'].str.split(' ', 1, expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing location of zipcode so it is placed next to city\n",
    "column_to_move = scraped_df.pop(\"zipcode\")\n",
    "\n",
    "# moving zipcode after state\n",
    "scraped_df.insert(4, \"zipcode\", column_to_move )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming columns\n",
    "scraped_df = scraped_df.rename(columns={\"Name\":\"ski_resort\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OnTheSnow - Scrape #2\n",
    "\n",
    "My initial scrape did not include ski run information, so I updated my initial scraping code and pulled ski run difficulty information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_scrape.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_scrape = second_scrape[['ski_resort','beginner_runs', 'intermediate_runs','advanced_runs', 'expert_runs']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging\n",
    "scraped_df = pd.merge(scraped_df, second_scrape, on=\"ski_resort\", how=\"left\")\n",
    "\n",
    "#replacing characters\n",
    "scraped_df = scraped_df.replace(\"%\", \"\", regex=True)\n",
    "\n",
    "#replacing more characters\n",
    "replace_vals = ['null', '-', \" \"]\n",
    "\n",
    "#replacing strings \n",
    "scraped_df['expert_runs'] = scraped_df['expert_runs'].replace(replace_vals, \"\", regex=True)\n",
    "\n",
    "#replacing empty cells with 0\n",
    "scraped_df['expert_runs'] = scraped_df['expert_runs'].replace(r'^\\s*$', 0, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling null values with 0 as a null value indicates 0 runs\n",
    "\n",
    "int_list = [\"beginner_runs\", \"intermediate_runs\", \"advanced_runs\", \"expert_runs\"]\n",
    "\n",
    "for x in int_list:\n",
    "    scraped_df[x] = scraped_df[x].fillna(0)\n",
    "\n",
    "#converting values to floats\n",
    "for x in int_list:\n",
    "    scraped_df[x].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking to see if there are any missing resorts in the scraped information to ensure all resorts have feature information\n",
    "final_ski_df.loc[~final_ski_df['ski_resort'].isin(scraped_df['ski_resort']),\n",
    "                         'ski_resort'].sort_values().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_from = [\"Anthony Lakes\", \"Big Squaw\",\"Brantling Ski\", \"Coffee Mill\", \"Mt. Shasta\", \"Mount Holly\"]\n",
    "replace_to = [\"Anthony Lakes Mountain\", 'Big Squaw Mountain Ski', \"Brantling Ski Slopes\", \"Coffee Mill Ski Snowboard\",\n",
    "               'Mt. Shasta Board Ski Park', \"Mt. Holly\"]\n",
    "\n",
    "scraped_df.replace(replace_from, replace_to, regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_df.loc[(scraped_df['ski_resort'] == \"Timberline\") & (scraped_df['state'] == \"West Virginia\"), 'ski_resort'] = \"Timberline Mountain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updating scraped df names to match the other dataframes\n",
    "scraped_df.loc[(scraped_df['ski_resort'] == \"Crystal Mountain\") & (scraped_df['state'] == \"Washington\"), 'ski_resort'] = \"Crystal Mountain Washington\"\n",
    "scraped_df.loc[(scraped_df['ski_resort'] == \"Crystal Mountain \") & (scraped_df['state'] == \"Michigan\"), 'ski_resort'] = \"Crystal Mountain Michigan\"\n",
    "\n",
    "#updating\n",
    "scraped_df.loc[(scraped_df['ski_resort'] == \"Timberline\") & (scraped_df['state'] == \"West Virginia\"), 'ski_resort'] = \"Timberline Mountain\"\n",
    "\n",
    "#updating scraped df names to match the other dataframes\n",
    "scraped_df.loc[(scraped_df['ski_resort'] == \"Powder Ridge\") & (scraped_df['state'] == \"Minnesota\"), 'ski_resort'] = \"Powder Ridge Minnesota\"\n",
    "scraped_df.loc[(scraped_df['ski_resort'] == \"Powder Ridge\") & (scraped_df['state'] == \"Connecticut\"), 'ski_resort'] = \"Powder Ridge Connecticut\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping duplicates\n",
    "scraped_df = scraped_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping duplicate names in ski_resort\n",
    "scraped_df = scraped_df.drop_duplicates(subset=['ski_resort'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking value counts of resorts to make sure there aren't duplicates or duplicate names\n",
    "scraped_df.ski_resort.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking to see if there are any missing resorts in the scraped information to ensure all resorts have feature information\n",
    "final_ski_df.loc[~final_ski_df['ski_resort'].isin(scraped_df['ski_resort']),\n",
    "                         'ski_resort'].sort_values().unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional feature engineering\n",
    "It is common for avid skiiers to purchase ski passes through companies that own a collective of mountains around the United States. I researched a current list of mountains of four of the most common ski passes, manually parsed through the names to update them to match the main dataframe, and then one hot encoded the values for each resort.\n",
    "\n",
    "I did attempt to use the fuzz to update the names, however many resorts have very similar names so it was not an effective way to update the resort names.\n",
    "\n",
    "- Epic Pass\n",
    "- Ikon Pass\n",
    "- Mountain Collective\n",
    "- Indy Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epic_list = [\"Stowe Mountain\", \"Okemo Mountain\", \"Hunter Mountain\", \"Mt. Snow\", \"Mt. Sunapee\",\"Wildcat Mountain\",\"Seven Springs\",\n",
    "             \"Attitash\", \"Jack Frost\", \"Crotched Mountain\", \"Laurel Mountain\",\n",
    "             \"Roundtop Mountain\", \"Whitetail\", \"Liberty\", \"Big Boulder\", \"Heavenly Mountain\", \"Northstar California\",\n",
    "             \"Kirkwood\", \"Stevens Pass\", \"Keystone\", \"Breckenridge\",\"Vail\", \"Park City Mountain\", \"Beaver Creek\",\n",
    "             \"Crested Butte Mountain\", \"Afton Alps\", \"Alpine Valley Ohio\", \"Boston Mills and Brandywine\",\"Hidden Valley\",\n",
    "             \"Mad River Mountain\", \"Mt. Brighton\", \"Paoli Peaks\", \"Snow Creek\",\"Wilmot Mountain\",\n",
    "            'Mt. Sunapee','Wildcat Mountain', 'Whitetail', 'Mt. Brighton', 'Wilmot Mountain']\n",
    "\n",
    "mtn_col_list = ['Arapahoe Basin', 'Aspen Snowmass', 'Jackson Hole', 'Mammoth Mountain', 'Snowbird',\n",
    "                            'Palisades Tahoe', 'Sugarbush','Taos Ski Valley', 'Alta', 'Big Sky', 'Sugar Bowl',\n",
    "                           'Sugarloaf', 'Sun Valley', 'Grand Targhee', 'Snowbasin']\n",
    "\n",
    "ikon_list = ['Palisades Tahoe', 'Mammoth Mountain', 'June Mountain', 'Bear Mountain', 'Snow Summit','Snow Valley',\n",
    "    'Sun Valley', 'Dollar Mountain', 'Crystal Mountain Washington', 'Alpental', 'The Summit at Snoqualmie',\n",
    "    'Mt. Bachelor', 'Schweitzer', 'Alyeska', 'Aspen Snowmass','Buttermilk', 'Steamboat', 'Winter Park',\n",
    "    'Copper Mountain', 'Arapahoe Basin', 'Eldora Mountain', 'Jackson Hole', 'Big Sky',\n",
    "    'Taos Ski Valley','Deer Valley', 'Solitude Mountain','Brighton','Alta', 'Snowbird',\n",
    "    'Snowbasin','Boyne Highlands', 'Boyne Mountain', 'Stratton Mountain', 'Sugarbush', 'Killington', 'Pico Mountain',\n",
    "    'Windham Mountain', 'Snowshoe Mountain', 'Sunday River','Sugarloaf','Loon Mountain']\n",
    "\n",
    "\n",
    "indy_list = ['Eaglecrest', 'Ski China Peak', 'Mt. Shasta Board Ski Park', 'Mountain High', 'Dodge Ridge',\n",
    "    'Hoodoo Ski Area', 'Mt. Ashland', 'Mt. Hood Meadows', '49 Degrees North',\n",
    "    'Hurricane Ridge Ski & Snowboard Area', 'Mission Ridge', 'Bluewood', 'White Pass',\n",
    "    'Castle Mountain Resort', 'Sunrise Park', 'Echo Mountain', 'Granby Ranch',\n",
    "    'Sunlight', 'Brundage Mountain', 'Kelly Canyon', 'Pomerelle Mountain', 'Silver Mountain', 'Soldier Mountain',\n",
    "    'Tamarack', 'Blacktail Mountain', 'Mountain', 'Red Lodge Mountain', 'Beaver Mountain',\n",
    "    'Powder Mountain', 'Antelope Butte', 'Snow King', 'White Pine Ski Resort',\n",
    "    'Seven Oaks', 'Sundown Mountain', 'Big Powderhorn Mountain', 'Caberfae Peaks Ski Golf',\n",
    "    'Crystal Mountain Michigan', 'Marquette Mountain', 'Nubs Nob', 'Pine Mountain',\n",
    "    'Schuss Mountain at Shanty Creek', 'Swiss Valley', 'Treetops Ski Resort',\n",
    "    'Buck Hill', 'Detroit Mountain Recreation Area', 'Lutsen Mountains', 'Mount Mankato',\n",
    "    'Powder Ridge Minnesota', 'Spirit Mountain', 'Terry Peak', 'Granite Peak',\n",
    "    'Little Switzerland', 'Nordic Mountain', 'The Rock Snowpark', 'Trollhaugen',\n",
    "    'Tyrol Basin', 'Mohawk Mountain', 'BigRockMountain',\n",
    "    'Rangeley Lakes Trail Center', 'Saddleback Mountain', 'Berkshire East',\n",
    "    'Black Mountain', 'Cannon Mountain', 'Pats Peak', 'Waterville Valley',\n",
    "    'Catamount Ski Ride Area', 'Greek Peak', 'Peekn Peak', 'Snow Ridge', 'Swain Resort',\n",
    "    'Titus Mountain', 'West Mountain', 'Catamount Outdoor Family Center', 'Bolton Valley',\n",
    "    'Jay Peak', 'Magic Mountain Vermont', 'Saskadena Six', 'Cataloochee',\n",
    "    'Blue Knob', 'Montage Mountain', 'Shawnee Mountain', 'Ski Sawmill',\n",
    "    'Tussey Mountain', 'Ober Gatlinburg Ski', 'Bryce', 'Massanutten',\n",
    "    'Canaan Valley', 'Winterplace Ski']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check to see if any of the names are wrong\n",
    "\n",
    "test_list = ikon_list + epic_list + mtn_col_list\n",
    "\n",
    "missing_values = [value for value in test_list if value not in scraped_df['ski_resort'].values]\n",
    "\n",
    "# Print the missing values\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check to see if any of the names are wrong\n",
    "\n",
    "missing_values = [value for value in indy_list if value not in scraped_df['ski_resort'].values]\n",
    "\n",
    "# Print the missing values\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making new column with 0\n",
    "scraped_df['epic'] = 0\n",
    "scraped_df['mountain_collective'] = 0\n",
    "scraped_df['ikon'] = 0\n",
    "scraped_df['indy'] = 0\n",
    "\n",
    "# adding 1 for each row value based on the ski resort pass lists\n",
    "scraped_df.loc[scraped_df['ski_resort'].isin(epic_list), 'epic'] = 1\n",
    "scraped_df.loc[scraped_df['ski_resort'].isin(mtn_col_list), 'mountain_collective'] = 1\n",
    "scraped_df.loc[scraped_df['ski_resort'].isin(ikon_list), 'ikon'] = 1\n",
    "scraped_df.loc[scraped_df['ski_resort'].isin(indy_list), 'indy'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Airbnb scraping file\n",
    "For the content based system, I will be scraping airbnb costs from each resort city. In the code below, I am making a new dataframe that I wll use that includes city, state, and ski resort information to pull information from airbnb.\n",
    "\n",
    "The scraping notebook can be found in the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making new dataframe\n",
    "city_df = pd.DataFrame()\n",
    "\n",
    "#saving city\n",
    "city_df['city'] = scraped_df['city']\n",
    "\n",
    "#saving state\n",
    "city_df['state'] = scraped_df['state']\n",
    "\n",
    "#saving ski resort name\n",
    "city_df['ski_resort'] = scraped_df['ski_resort']\n",
    "\n",
    "#saving \n",
    "#city_df.to_csv(\"cleaned_data_exports/location_for_scraping_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving city names for scraping\n",
    "city_df['location'] = scraped_df[['city', 'state']].agg(', '.join, axis=1)\n",
    "\n",
    "#saving unique cities\n",
    "city_df['location'] = pd.DataFrame(city_df['location'].unique())\n",
    "\n",
    "#dropping nulls\n",
    "city_df = city_df.dropna()\n",
    "\n",
    "#saving\n",
    "#city_df.to_csv(\"cleaned_data_exports/city_names_for_scraping_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving final merged dataframe for content based system\n",
    "#merged_df.to_csv(\"cleaned_data_exports/user_df_model.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airbnb Scrape Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing .csv that I scraped from Airbnb. This includes prices of airbnb's pulled from the first two pages (32 results) for airbnbs with a max of **2 guests** and max of **4 guests**. I pulled this information to provide cost information to those planning ski trips, as high lodging and ticket costs are a barrier to entry while planning ski trips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_2_airbnb_mean_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_2_airbnb_mean_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making lists of all dataframes from airbnb scrape\n",
    "two_guest_list = [dec_2_airbnb_mean_final,jan_2_airbnb_mean_final,feb_2_airbnb_mean_final,\n",
    "                  mar_2_airbnb_mean_final, apr_2_airbnb_mean_final,\n",
    "                  may_2_airbnb_mean_final]\n",
    "\n",
    "four_guest_list = [dec_4_airbnb_mean_final,jan_4_airbnb_mean_final,feb_4_airbnb_mean_final,\n",
    "                  mar_4_airbnb_mean_final, apr_4_airbnb_mean_final,\n",
    "                  may_4_airbnb_mean_final]\n",
    "\n",
    "month_list = ['dec', 'jan', 'feb', 'mar', 'apr', 'may']\n",
    "\n",
    "for x, y, z in zip(two_guest_list, four_guest_list, month_list):\n",
    "    \n",
    "    dict_2 = {'mean': z + '_mean_2_guests',\n",
    "              'min': z + '_min_2_guests','max': z + '_max_2_guests'}\n",
    "\n",
    "    dict_4 = {'mean': z +'_mean_4_guests',\n",
    "              'min': z +'_min_4_guests','max': z +'_max_4_guests'}\n",
    "    \n",
    "    #renaming column names based on scrape\n",
    "    x.rename(columns=dict_2, inplace=True)\n",
    "    y.rename(columns=dict_4, inplace=True)\n",
    "    \n",
    "    #saving as new dataframe\n",
    "    x.drop(columns=['count', 'std', '25%', '50%', '75%', 'Unnamed: 0'], inplace=True)\n",
    "    y.drop(columns=['count', 'std', '25%', '50%', '75%', 'Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging all months from the list of 2 guest airbnbs \n",
    "for df in two_guest_list[1:]:\n",
    "    dec_2_airbnb_mean_final = pd.merge(dec_2_airbnb_mean_final, df, on='ski_resort')\n",
    "\n",
    "#mergins all months from the list of 4 guest airbnbs\n",
    "for df in four_guest_list:\n",
    "    dec_2_airbnb_mean_final = pd.merge(dec_2_airbnb_mean_final, df, on='ski_resort')\n",
    "    \n",
    "#saving off the list as a new dataframe\n",
    "airbnb_df = dec_2_airbnb_mean_final.copy()\n",
    "\n",
    "#dropping duplicates\n",
    "airbnb_df = airbnb_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspecting value counts\n",
    "airbnb_df['ski_resort'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking to see which ski resort names different between the main feature df and the airbnb df\n",
    "scraped_df.loc[~scraped_df['ski_resort'].isin(airbnb_df['ski_resort']),\n",
    "                         'ski_resort'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming resorts that have the same name\n",
    "airbnb_df.replace(\"Crystal Mountain\", \"Crystal Mountain Washington\", regex=True, inplace=True)\n",
    "\n",
    "#renaming resorts that have the same name\n",
    "airbnb_df.replace(\"Powder Ridge\", \"Powder Ridge Minnesota\", regex=True, inplace=True)\n",
    "\n",
    "airbnb_df.loc[(airbnb_df['ski_resort'] == \"Crystal Mountain Washington \") & (airbnb_df['dec_min_2_guests'] == 40), 'ski_resort'] = \"Crystal Mountain Michigan\"\n",
    "\n",
    "airbnb_df.replace(\"Powder Ridge\", \"Powder Ridge Connecticut\", inplace=True)\n",
    "\n",
    "airbnb_df.replace('Mt. Bhelor', 'Mt. Bachelor', inplace=True)\n",
    "\n",
    "airbnb_df.loc[(airbnb_df['ski_resort'] == 'Timberline') & (airbnb_df['dec_min_2_guests'] == 75.0), 'ski_resort'] = \"Timberline Mountain\"\n",
    "\n",
    "#batch renaming mountains to match the user df\n",
    "rename_list = [\"Anthony Lakes\", \"Mount Holly\", \"Mt. Shasta\", \"Coffee ll\", \"Brantling Ski\", 'Big Squaw']\n",
    "rename_to = ['Anthony Lakes Mountain', 'Mt. Holly','Mt. Shasta Board Ski Park', 'Coffee Mill Ski Snowboard',\n",
    "       'Brantling Ski Slopes', 'Big Squaw Mountain Ski']\n",
    "\n",
    "airbnb_df.replace(rename_list, rename_to, regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking to see which ski resort names different between the main feature df and the airbnb df\n",
    "scraped_df.loc[~scraped_df['ski_resort'].isin(airbnb_df['ski_resort']),\n",
    "                         'ski_resort'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging reviews with features\n",
    "I will be combining the cleaned dataframes from below that will be used for the collaborative model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #merging final review dataframe and scraped data\n",
    "# merged_df = pd.merge(final_ski_df, scraped_df, on=\"ski_resort\", how='left')\n",
    "\n",
    "# #dropping columns\n",
    "# merged_df = merged_df.drop(columns=\"state_y\")\n",
    "\n",
    "# #renaming columns\n",
    "# merged_df = merged_df.rename(columns={\"state_x\":\"state\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging airbnb with feature dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging airbnb and feature dataframes\n",
    "content_df = pd.merge(scraped_df, airbnb_df, on=\"ski_resort\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Geocoding API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the final .csv from the Google Geocoding API pull. The code for this can be found in the **scraping_ipynb** file.\n",
    "\n",
    "Google's Geocoding API is a service that accepts a place as an address, latitude and longitude coordinates, or Place ID. It converts the address into latitude and longitude coordinates and a Place ID, or converts latitude and longitude coordinates or a Place ID into an address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude_df.drop(columns=\"Unnamed: 0\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_df.loc[~content_df['ski_resort'].isin(latitude_df['ski_resort']),\n",
    "                         'ski_resort'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updating Timberline because there are multiple similar resorts\n",
    "latitude_df.loc[(latitude_df['ski_resort'] == 'Timberline') & (latitude_df['full_address'] == \"HC 70 Box 488, Davis, West Virginia\"), 'ski_resort'] = \"Timberline Mountain\"\n",
    "\n",
    "#batch renaming\n",
    "lat_list = [\"Anthony Lakes\", \"Mount Holly\", \"Mt. Shasta\", \"Coffee ll\", \"Brantling Ski\", \"Big Squaw\"]\n",
    "lat_replace = ['Anthony Lakes Mountain', 'Mt. Holly','Mt. Shasta Board Ski Park',\n",
    "               'Coffee Mill Ski Snowboard','Brantling Ski Slopes', 'Big Squaw Mountain Ski']\n",
    "\n",
    "latitude_df.replace(lat_list, lat_replace, regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confiring all names are consistent besides two remaining values that don't exist in the other dataframe\n",
    "content_df.loc[~content_df['ski_resort'].isin(latitude_df['ski_resort']),\n",
    "                         'ski_resort'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging with feature dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging latitude df with final content_df\n",
    "content_df = pd.merge(content_df, latitude_df, on=\"ski_resort\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Additional Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lift_list = ['gondolas_and_trams','fastEight','highSpeedSixes','quadChairs','tripleChairs','doubleChairs',\n",
    "             'surfeLifts']\n",
    "\n",
    "for x in lift_list:\n",
    "    content_df[x] = content_df[x].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_df['gondolas_and_trams'] = content_df['gondolas_and_trams'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making new column that totals the lift sum\n",
    "content_df['total_lifts'] = 0 \n",
    "\n",
    "#adding columns\n",
    "content_df['total_lifts'] = content_df['gondolas_and_trams'] + content_df['fastEight'] + content_df['highSpeedSixes'] + content_df['quadChairs'] + content_df['tripleChairs'] + content_df['doubleChairs'] + content_df['surfeLifts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_df.drop_duplicates(subset=\"ski_resort\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving final cleaned scraped df\n",
    "#content_df.to_csv(\"cleaned_data_exports/scraped_feature_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratings by resort distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at the most reviewed mountains\n",
    "top_5_reviewed_resorts = pd.DataFrame(final_ski_df['ski_resort'].value_counts().reset_index()).head(5)\n",
    "top_5_reviewed_resorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using plotly to plot the top reviewers\n",
    "fig = px.bar(top_5_reviewed_resorts, x=\"index\", y=\"ski_resort\")\n",
    "fig.update_layout(title_text='Most Reviewed Mountains',\n",
    "                  title_x=0.5,\n",
    "                  xaxis_title=\"Resort\",\n",
    "                  yaxis_title=\"Review Count\",\n",
    "                 plot_bgcolor='white')\n",
    "fig.update_traces(marker_color = \"#00b5ff\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rating distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an imbalance in rating distrubutions, however the breakdown of ratings is not in line with typical user bias where ratings are either on the high or low scale. This imbalance will most likely end up affecting the performance of our modeling, but we can choose an algorithm that works best for the type of data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making dataframe of rating counts to compare distribution of ratings\n",
    "top_ratings = pd.DataFrame(final_ski_df[\"rating\"].value_counts(ascending=False).head(15))\n",
    "top_ratings = top_ratings.reset_index()\n",
    "top_ratings = top_ratings.rename(columns={\"rating\":\"rating_count\"})\n",
    "top_ratings = top_ratings.rename(columns={\"index\":\"rating\"})\n",
    "\n",
    "#making user_id a string for plotting\n",
    "top_ratings['rating'] = top_ratings['rating'].astype(str)\n",
    "\n",
    "# Calculate the percentage of each rating count\n",
    "top_ratings['rating_percentage'] = (top_ratings['rating_count'] / top_ratings['rating_count'].sum()) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using plotly to plot the top featurescolor=\n",
    "fig = px.bar(top_ratings, x=\"rating\", y=\"rating_percentage\",\n",
    "             text=\"rating_percentage\")\n",
    "fig.update_layout(title_text='Rating Distribution',\n",
    "                  title_x=0.5,\n",
    "                  xaxis_title=\"Rating\",\n",
    "                  yaxis_title=\"Rating %\",\n",
    "                 plot_bgcolor='white',\n",
    "                 font=dict(size=14))\n",
    "fig.update_traces(marker_color = \"#00b5ff\", texttemplate='%{text:.1s}%', textposition='outside')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly Snowfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_list = [\"NovSnow\", \"DecSnow\", \"JanSnow\", \"FebSnow\", \"MarSnow\", \"AprSnow\"]\n",
    "snow_names = ['November', 'December', 'January', 'February', 'March', 'April']\n",
    "\n",
    "monthly_mean = content_df[snow_list].mean(skipna=True)\n",
    "\n",
    "monthly_snowfall = pd.DataFrame({'month': snow_names, 'mean_snowfall': monthly_mean.values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_snowfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using plotly to plot the top featurescolor=\n",
    "fig = px.bar(monthly_snowfall, x=\"month\", y=\"mean_snowfall\")\n",
    "fig.update_layout(title_text='2022 US Average Snowfall',\n",
    "                  title_x=0.5,\n",
    "                  xaxis_title=\"Month\",\n",
    "                  yaxis_title=\"Snow (in)\",\n",
    "                 plot_bgcolor='white',\n",
    "                 font=dict(size=14))\n",
    "fig.update_traces(marker_color = \"#00b5ff\",textposition='outside')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airbnb Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using plotly to plot the top featurescolor=\n",
    "fig = px.bar(content_df.head(), x=\"ski_resort\", y=[\"dec_min_2_guests\", \"dec_min_4_guests\"],\n",
    "            width=1000, height=500)\n",
    "fig.update_layout(title_text='December Airbnb Costs',\n",
    "                  title_x=0.5,\n",
    "                  xaxis_title=\"Ski Resort\",\n",
    "                  yaxis_title=\"Nightly Price ($)\",\n",
    "                 plot_bgcolor='white',\n",
    "                 font=dict(size=14),\n",
    "                 barmode='group')\n",
    "\n",
    "newnames = {'dec_min_2_guests':'2 Guest Max', 'dec_min_4_guests': '4 Guest Max'}\n",
    "fig.for_each_trace(lambda t: t.update(name = newnames[t.name],\n",
    "                                      legendgroup = newnames[t.name],\n",
    "                                      hovertemplate = t.hovertemplate.replace(t.name, newnames[t.name])\n",
    "                                     )\n",
    "                  )\n",
    "\n",
    "fig.update_traces(textposition='outside')               \n",
    "              \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elevation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mountain_elevation(resort_name):\n",
    "\n",
    "    resort_df = content_df.loc[content_df['ski_resort'] == resort_name]\n",
    "    \n",
    "    # elevation\n",
    "    base_elevation = resort_df['base'].values[0]\n",
    "    summit_elevation = resort_df['sumt'].values[0]\n",
    "\n",
    "    # traving elevation\n",
    "    elev_trace = go.Scatter(x=[\"base\", \"summit\", \"drop\"], y=[base_elevation, summit_elevation, base_elevation], mode='lines', line=dict(color='blue'))\n",
    "\n",
    "    # displaying plot\n",
    "    layout = go.Layout(\n",
    "        title='Elevation Change',\n",
    "        yaxis=dict(title='Elevation'),\n",
    "        plot_bgcolor='white',\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    # making figure\n",
    "    fig = go.Figure(data=[elev_trace], layout=layout)\n",
    "\n",
    "    # Showing the line plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mountain_elevation(\"Arapahoe Basin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review Dataset\n",
    "After cleaning and analyzing the data, there are **662 users, 275 resorts, and 2795 total reviews**. There is an imbalance in the reviews, however our final recommendation system will be a hybrid-cascade model, so this will help balance out the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_users = len(final_ski_df['user_name'].unique())\n",
    "unique_resorts = len(final_ski_df['ski_resort'].unique())\n",
    "total_reviews = len(final_ski_df)\n",
    "\n",
    "print(\"Number of unique users:\", unique_users)\n",
    "print(\"Number of unique resorts:\", unique_resorts)\n",
    "print(\"Number of reviews:\", total_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review Dataset\n",
    "After cleaning the scraped data from OnTheSnow, Google Geocoding API, and Airbnb, there are **329 resorts** and **86 columns** in the final dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_resorts = len(content_df['ski_resort'].unique())\n",
    "unique_features = len(content_df.columns)\n",
    "\n",
    "print(\"Number of resorts:\", unique_resorts)\n",
    "print(\"Number of features:\", unique_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step will be to begin modeling to create the recommendation system. The two main dataframes from this notebook will be used are listed below:\n",
    "\n",
    "- **Collaborative Modeling** - cleaned_data_exports/user_df_model.csv\n",
    "- **Content/Cascade Hybrid Modeling** - cleaned_data_exports/scraped_feature_df.csv\n",
    "\n",
    "The collaborative model will be saved in a separate notebook than the final content and hybrid based models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
